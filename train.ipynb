{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Biting The Bytes: Transformers For Diacritic Restoration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b> Istanbul Technical University - Natural Language Processing - Spring 2024 </b>\n",
    "\n",
    "Emircan Erol - 150200324 <br>\n",
    "erole20@itu.edu.tr\n",
    "\n",
    "Muhammed R√º≈üen Birben - 150220755 <br>\n",
    "birben20@itu.edu.tr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The notebook has been run many times to get the results. We've iteratively selected checkpoint models (saved the best ones), modified the hyperparameters, and changed the dataset. The base model is [`google-byt5-small`](https://huggingface.co/google/byt5-small)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The resources used in this projects can be found in the following drive folder:\n",
    "\n",
    "[` Google Drive Link üóÇÔ∏è`](https://drive.google.com/drive/folders/1nfAvnj_-EB4FMa83mUCtGNel9DkdC0PL?usp=sharing)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports and Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from random import shuffle\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "import torch\n",
    "import wandb\n",
    "import json\n",
    "import os\n",
    "from os.path import exists as is_path_exists\n",
    "\n",
    "LR = 2 ** (-12)\n",
    "CHECKPOINT = None # 'models/peft32-train32-256-base-trtw1e-train-512-1024-base-trw-6+kelimeler-2-6200+6300tr3600-train128tr128-1000.pt'\n",
    "MIN_LEN = 0\n",
    "MAX_LEN = 32\n",
    "model_id = 'google/byt5-small' # needed if training from scratch\n",
    "BATCH_SIZE = 64\n",
    "enc = 'utf-8'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "metadata": {}
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checkpoint None does not exist. Training from scratch.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of T5ForTokenClassification were not initialized from the model checkpoint at google/byt5-base and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 2,657,282 || all params: 417,363,844 || trainable%: 0.6366823667648605\n"
     ]
    }
   ],
   "source": [
    "if CHECKPOINT:\n",
    "    assert is_path_exists(CHECKPOINT)\n",
    "    print(f'Checkpoint {CHECKPOINT} exists. Loading model from checkpoint.')\n",
    "    model = torch.load(CHECKPOINT)\n",
    "    # turn on peft\n",
    "    #model = prepare_model_for_kbit_training(model)\n",
    "    #model = get_peft_model(model, peft_config)\n",
    "else:\n",
    "    # peft is used to redce the trainable parameter size of the base model\n",
    "    from peft import LoraConfig, prepare_model_for_kbit_training, get_peft_model\n",
    "    from turbot5 import T5ForTokenClassification\n",
    "    from transformers import BitsAndBytesConfig\n",
    "    \n",
    "    bnb_config = BitsAndBytesConfig(\n",
    "        load_in_4bit=True,\n",
    "        bnb_4bit_quant_type=\"nf4\",\n",
    "        bnb_4bit_use_double_quant=True,\n",
    "        bnb_4bit_compute_dtype=torch.float16,\n",
    "    )\n",
    "\n",
    "    LORA_ALPHA = 64\n",
    "    LORA_DROPOUT = 0.125\n",
    "    R = 32\n",
    "\n",
    "    peft_config = LoraConfig(\n",
    "        lora_alpha=LORA_ALPHA,\n",
    "        lora_dropout=LORA_DROPOUT,\n",
    "        r=R,\n",
    "        bias=\"none\",\n",
    "        task_type='TOKEN_CLS',\n",
    "        target_modules = ['k', 'q', 'v', 'o']\n",
    "    )\n",
    "    print(f'Checkpoint {CHECKPOINT} does not exist. Training from scratch.')\n",
    "    \n",
    "    model = T5ForTokenClassification.from_pretrained(model_id,\n",
    "                                                    num_labels=2,\n",
    "                                                    torch_dtype=torch.bfloat16,\n",
    "                                                    quantization_config=bnb_config,\n",
    "                                                    device_map=\"auto\",\n",
    "                                                    attention_type = 'flash')\n",
    "    model = prepare_model_for_kbit_training(model)\n",
    "    model = get_peft_model(model, peft_config)\n",
    "\n",
    "model.print_trainable_parameters()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparing the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that the conversion of main training data to .json is not provided, it is very similar with the conversion of the text, just substitute the diactritic characters with their non-diactritic counterparts. and convert to .json.\n",
    "\n",
    "Something like this:\n",
    "\n",
    "```python\n",
    "\n",
    "def process_data(input_file, output_file, answer_file):\n",
    "    # Read input file (CSV or JSONL)\n",
    "    if input_file.endswith('.csv'):\n",
    "        df = pd.read_csv(input_file)\n",
    "    elif input_file.endswith('.jsonl'):\n",
    "        df = pd.read_json(input_file, lines=True, encoding='utf-8-sig')\n",
    "    else:\n",
    "        raise ValueError(\"Unsupported input file format. Use CSV or JSONL.\")\n",
    "\n",
    "    # Shuffle the dataframe\n",
    "    df = df.sample(frac=1).reset_index(drop=True)\n",
    "\n",
    "    # Rename columns\n",
    "    df = df.rename(columns={'Sentence': 'output'})\n",
    "\n",
    "\n",
    "    # Add an input non-diacritic version of the output\n",
    "    # This is the input for the model\n",
    "    df['input'] = df['output'].apply(asciify_turkish_chars)    \n",
    "\n",
    "    # Reorder columns\n",
    "    df = df[['ID', 'input', 'output']]\n",
    "\n",
    "    # Save the processed data to the output file (JSONL)\n",
    "    df.to_json(output_file, orient='records', lines=True)\n",
    "\n",
    "    # Save the answer data to the answer file (CSV)\n",
    "    df[['ID', 'output']].to_csv(answer_file, index=False)\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def asciify_turkish_chars(text):\n",
    "    \"\"\"\n",
    "    Removes diacritics from Turkish text.\n",
    "    Args:\n",
    "        text (str): Turkish text.\n",
    "    Returns:\n",
    "        text: Turkish text without diacritics.\n",
    "    \"\"\"\n",
    "    turkish_chars = {\n",
    "        '√ß': 'c',\n",
    "        'ƒü': 'g',\n",
    "        'ƒ±': 'i',\n",
    "        '√∂': 'o',\n",
    "        '≈ü': 's',\n",
    "        '√º': 'u',\n",
    "        '√á': 'C',\n",
    "        'ƒû': 'G',\n",
    "        'ƒ∞': 'I',\n",
    "        '√ñ': 'O',\n",
    "        '≈û': 'S',\n",
    "        '√ú': 'U'\n",
    "    }\n",
    "    \n",
    "    for char, replacement in turkish_chars.items():\n",
    "        text = text.replace(char, replacement)\n",
    "    \n",
    "    return text\n",
    "\n",
    "def txt_to_input_output(fp, skip=500_000, split='all'):\n",
    "    \"\"\"\n",
    "    Reads a text file and writes it to a jsonl file. \n",
    "    So that it can be used as a dataset.\n",
    "    Args:\n",
    "        fp (str): File path of the text file.\n",
    "        skip (int): Number of lines to skip.\n",
    "        split (int): Number of lines to read.\n",
    "    \"\"\"\n",
    "    sentences = []\n",
    "    dict_data = []\n",
    "    with open(fp, 'r') as f:\n",
    "        for line in f:\n",
    "            if skip:\n",
    "                skip -= 1\n",
    "                continue\n",
    "            \n",
    "            elif line == '\\n': continue\n",
    "\n",
    "            elif split == 'all':\n",
    "                sentences.append(line.strip('\\n'))\n",
    "\n",
    "            elif MIN_LEN <= len(line) <= MAX_LEN:\n",
    "                sentences.append(line.strip('\\n'))\n",
    "                split -= 1\n",
    "                if split == 0: break\n",
    "\n",
    "        dict_data = [{'input': asciify_turkish_chars(i), 'output': i} for i in sentences]\n",
    "\n",
    "    with open('data.jsonl', mode='w', encoding='utf-8') as f:\n",
    "        for w_dict in dict_data:\n",
    "            json.dump(w_dict, f)\n",
    "            f.write('\\n')\n",
    "\n",
    "# running the function if we need to create dataset from scratch\n",
    "# txt_to_input_output('turkce_kelime_listesi.txt', skip=0, split='all')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "metadata": {}
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of samples: 76186\n"
     ]
    }
   ],
   "source": [
    "# Read data with a minimum and maximum length\n",
    "path = 'data.jsonl' # 'data.jsonl'\n",
    "\n",
    "data = []\n",
    "with open(path, 'r', encoding=enc) as f:\n",
    "    for jline in f.readlines():\n",
    "        line = json.loads(jline)\n",
    "        n = len(line['input'])\n",
    "        if MIN_LEN <= n <= MAX_LEN:\n",
    "            data.append(line)\n",
    "\n",
    "print(f'Number of samples: {len(data)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "shuffle(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: ovunmak\n",
      "Output: ovunmak\n",
      "Input: mujdecilik\n",
      "Output: m√ºjdecilik\n",
      "Input: Ardesen \n",
      "Output: Arde≈üen \n",
      "Input: bir alay\n",
      "Output: bir alay\n",
      "Input: ihtiyat akcesi\n",
      "Output: ihtiyat ak√ßesi\n"
     ]
    }
   ],
   "source": [
    "# print some samples\n",
    "for i in range(5):\n",
    "    print(f\"Input: {data[i]['input']}\\nOutput: {data[i]['output']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "metadata": {}
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of batches: 1190\n"
     ]
    }
   ],
   "source": [
    "def mask_label(data, batch_size=BATCH_SIZE):\n",
    "    \"\"\"\n",
    "    Masks the padded tokens in the input and creates batches.\n",
    "    Args:\n",
    "        data (list): List of dictionaries.\n",
    "        batch_size (int): Batch size.\n",
    "    Returns:\n",
    "        batches (list): List of dictionaries.\n",
    "        The dictionaries contain the following keys: 'input_ids', 'labels', 'attention_mask'\n",
    "    \"\"\"\n",
    "    batches = list()\n",
    "    \n",
    "    dataset = list()\n",
    "    for idx, sample in enumerate(data):\n",
    "        inp = list(sample['input'])\n",
    "        gold = list(sample['output'])\n",
    "        \n",
    "        new_sample = dict()\n",
    "\n",
    "        label = []\n",
    "        \n",
    "        # create labels\n",
    "        for i, j in zip(inp, gold):\n",
    "            n = len(i.encode('utf-8'))\n",
    "            label.extend([i != j] * n)\n",
    "\n",
    "        new_sample['labels'] = label #¬†labels\n",
    "        new_sample['input_ids'] = [i + 3 for i in sample['input'].encode('utf-8')] #¬†input_ids\n",
    "        \n",
    "        assert len(label) == len(new_sample['input_ids'])\n",
    "        new_sample['input_ids'].append(0) # eos token\n",
    "\n",
    "        dataset.append(new_sample)\n",
    "        if idx and not (idx % batch_size):\n",
    "            batch = dict()\n",
    "            max_size = 0\n",
    "            for i in dataset[idx-batch_size: idx]:\n",
    "                length = len(i['input_ids'])\n",
    "                if length > max_size: max_size = length\n",
    "\n",
    "            input_ids = list()\n",
    "            labels = list()\n",
    "\n",
    "            # padding\n",
    "            for i in dataset[idx-batch_size: idx]:\n",
    "                i['labels'].extend([False] * (max_size - len(i['labels'])))\n",
    "                i['input_ids'].extend([0] * (max_size - len(i['input_ids'])))\n",
    "                input_ids.append(i['input_ids'])\n",
    "                labels.append(i['labels'])\n",
    "\n",
    "            batch['labels'] = torch.tensor(labels, dtype=torch.int64)\n",
    "            batch['input_ids'] = torch.tensor(input_ids, dtype=torch.int64)\n",
    "\n",
    "            # attention mask: 1 for real tokens, 0 for padding\n",
    "            batch['attention_mask'] = torch.ones_like(batch['input_ids'], dtype=torch.int64)\n",
    "            batch['attention_mask'][batch['input_ids'] == 0] = 0\n",
    "            batches.append(batch)\n",
    "\n",
    "    return batches\n",
    "\n",
    "ds = mask_label(data)\n",
    "print(f'Number of batches: {len(ds)}')\n",
    "assert len(ds) > 0\n",
    "del data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "def test_mask(data):\n",
    "    \"\"\"\n",
    "    Masks the padded tokens in the input.\n",
    "    Args:\n",
    "        data (list): List of strings.\n",
    "    Returns:\n",
    "        dataset (list): List of dictionaries.\n",
    "    \"\"\"\n",
    "\n",
    "    dataset = list()\n",
    "    for sample in data:        \n",
    "        new_sample = dict()\n",
    "\n",
    "        input_tokens = [i + 3 for i in sample.encode('utf-8')]\n",
    "        input_tokens.append(0) # eos token\n",
    "        new_sample['input_ids'] = torch.tensor([input_tokens], dtype=torch.int64)\n",
    "        \n",
    "        # Create attention mask\n",
    "        attention_mask = [1] * len(input_tokens)  # Attend to all tokens\n",
    "        new_sample['attention_mask'] = torch.tensor([attention_mask], dtype=torch.int64)\n",
    "        \n",
    "        dataset.append(new_sample)\n",
    "        \n",
    "    return dataset\n",
    "\n",
    "def rewrite(model, data):\n",
    "    \"\"\"\n",
    "    Rewrites the input text with the model.\n",
    "    Args:\n",
    "        model (torch.nn.Module): Model.\n",
    "        data (dict): Dictionary containing 'input_ids' and 'attention_mask'.\n",
    "    Returns:\n",
    "        output (str): Rewritten text.\n",
    "    \"\"\"\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for k, v in data.items():\n",
    "            data[k] = data[k].to(model.device)\n",
    "        pred = torch.argmax(model(**data).logits, dim=2)\n",
    "        \n",
    "    output = list() # save the indices of the characters as list of integers\n",
    "    \n",
    "    # Conversion table for Turkish characters {100: [300, 350], ...}\n",
    "    en2tr = {en: tr for tr, en in zip(list(map(list, map(str.encode, list('√úƒ∞ƒû≈û√á√ñ√ºƒ±ƒü≈ü√ß√∂')))), list(map(ord, list('UIGSCOuigsco'))))}\n",
    "\n",
    "    for inp, lab in zip((data['input_ids'] - 3)[0].tolist(), pred[0].tolist()):\n",
    "        if lab and inp in en2tr:\n",
    "            # if the model predicts a diacritic, replace it with the corresponding Turkish character\n",
    "            output.extend(en2tr[inp])\n",
    "        elif inp >= 0: output.append(inp)\n",
    "    return bytes(output).decode()\n",
    "\n",
    "def submission(model=model, split=None, path='test.jsonl', output_path='submission.csv'):\n",
    "    \"\"\"\n",
    "    Creates a submission file.\n",
    "    Args:\n",
    "        model (torch.nn.Module): Model.\n",
    "        path (str): Path of the test data.\n",
    "        output_path (str): Path of the submission file.\n",
    "    Returns:\n",
    "        df (pd.DataFrame): Dataframe containing the submission.\n",
    "    \"\"\"\n",
    "    \n",
    "    predictions = list()\n",
    "    with open(path, 'r', encoding=enc) as f:\n",
    "        data = [json.loads(jline) for jline in f.readlines()[:split]]\n",
    "        ds = test_mask([i['input'] for i in data])\n",
    "        for sample in tqdm(ds):\n",
    "            predictions.append([rewrite(model, sample)])\n",
    "    \n",
    "    df = pd.DataFrame(predictions, columns=['output'])\n",
    "\n",
    "    # take ids from data\n",
    "    df['ID'] = [i['ID'] for i in data]\n",
    "    df.rename(columns={'output': 'Sentence'}, inplace=True)\n",
    "    df = df[['ID', 'Sentence']]\n",
    "    df.to_csv(output_path, index=False, encoding=enc)\n",
    "    \n",
    "    return df\n",
    "\n",
    "\n",
    "def acc_overall(test_result, testgold, verbose=True):\n",
    "    \"\"\"\n",
    "    Calculates the overall accuracy. Using the formula used in the competition.\n",
    "    Args:\n",
    "        test_result (list): List of strings.\n",
    "        testgold (list): List of strings.\n",
    "        verbose (bool): If True, prints the incorrect words.\n",
    "    Returns:\n",
    "        float: Overall accuracy.\n",
    "    \"\"\"\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    # count number of correctly diacritized words\n",
    "    for i in range(len(testgold)):\n",
    "        golds = testgold[i].split()\n",
    "        results = test_result[i].split()\n",
    "        for m in range(len(golds)):\n",
    "            if results[m] == golds[m]:\n",
    "                correct += 1\n",
    "            elif verbose:\n",
    "                print(results[m], golds[m]) #¬†print the incorrect words, handy to see the errors\n",
    "            total += 1\n",
    "    \n",
    "    return correct / total"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*setting weights and biases service for tracking*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.16.6"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>c:\\Users\\emirc\\Desktop\\NLP Project\\wandb\\run-20240506_192946-v8nrjjhe</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/itu-nlp/NLP%20Project/runs/v8nrjjhe' target=\"_blank\">pious-aardvark-94</a></strong> to <a href='https://wandb.ai/itu-nlp/NLP%20Project' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/itu-nlp/NLP%20Project' target=\"_blank\">https://wandb.ai/itu-nlp/NLP%20Project</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/itu-nlp/NLP%20Project/runs/v8nrjjhe' target=\"_blank\">https://wandb.ai/itu-nlp/NLP%20Project/runs/v8nrjjhe</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "os.environ[\"WANDB__SERVICE_WAIT\"] = \"300\"\n",
    "os.environ[\"WANDB_PROJECT\"]=\"NLP Project\"\n",
    "os.environ[\"WANDB_LOG_MODEL\"]=\"true\"\n",
    "os.environ[\"WANDB_WATCH\"]=\"true\"\n",
    "os.environ[\"WANDB_NOTEBOOK_NAME\"]=\"token_classification.ipynb\"\n",
    "run = wandb.init()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "metadata": {}
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "243e844adb8049048f110e6525d884e0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.133 MB of 0.133 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>accuracy</td><td>‚ñÑ‚ñÉ‚ñÜ‚ñÑ‚ñÇ‚ñÑ‚ñÖ‚ñÇ‚ñÖ‚ñá‚ñÇ‚ñÉ‚ñÖ‚ñÑ‚ñÜ‚ñÜ‚ñÖ‚ñÜ‚ñá‚ñÖ‚ñÅ‚ñÖ‚ñÖ‚ñÜ‚ñÑ‚ñà‚ñÜ‚ñÜ‚ñÇ‚ñá‚ñÑ‚ñÑ‚ñÜ‚ñÉ‚ñá‚ñÜ‚ñÖ‚ñÉ‚ñÉ‚ñÖ</td></tr><tr><td>batch</td><td>‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà</td></tr><tr><td>f1</td><td>‚ñÇ‚ñÉ‚ñÑ‚ñÅ‚ñÇ‚ñÖ‚ñÑ‚ñÇ‚ñÜ‚ñÜ‚ñÇ‚ñÑ‚ñÅ‚ñÑ‚ñÖ‚ñÖ‚ñÜ‚ñá‚ñÜ‚ñÑ‚ñÅ‚ñÉ‚ñÑ‚ñÑ‚ñÇ‚ñÜ‚ñÖ‚ñá‚ñÉ‚ñà‚ñÜ‚ñÜ‚ñÜ‚ñÉ‚ñá‚ñÜ‚ñá‚ñÑ‚ñÉ‚ñá</td></tr><tr><td>loss</td><td>‚ñá‚ñÉ‚ñÜ‚ñÜ‚ñà‚ñÉ‚ñÖ‚ñÖ‚ñÇ‚ñÑ‚ñÖ‚ñÜ‚ñÖ‚ñÜ‚ñÖ‚ñÜ‚ñÇ‚ñÑ‚ñÇ‚ñÜ‚ñá‚ñÖ‚ñá‚ñÑ‚ñÖ‚ñÜ‚ñÖ‚ñÑ‚ñÖ‚ñÅ‚ñÇ‚ñÉ‚ñÇ‚ñÑ‚ñÅ‚ñÉ‚ñÇ‚ñÇ‚ñÑ‚ñÅ</td></tr><tr><td>precision</td><td>‚ñÉ‚ñÜ‚ñÉ‚ñÇ‚ñÜ‚ñá‚ñÖ‚ñá‚ñÜ‚ñÑ‚ñÖ‚ñÜ‚ñÅ‚ñÜ‚ñÖ‚ñÑ‚ñÜ‚ñÜ‚ñÑ‚ñÑ‚ñá‚ñÉ‚ñÑ‚ñÉ‚ñÉ‚ñÉ‚ñÖ‚ñá‚ñá‚ñÜ‚ñà‚ñà‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñá‚ñá‚ñÖ‚ñá</td></tr><tr><td>recall</td><td>‚ñÑ‚ñÉ‚ñÜ‚ñÑ‚ñÇ‚ñÑ‚ñÖ‚ñÇ‚ñÖ‚ñá‚ñÇ‚ñÉ‚ñÖ‚ñÑ‚ñÜ‚ñÜ‚ñÖ‚ñÜ‚ñá‚ñÖ‚ñÅ‚ñÖ‚ñÖ‚ñÜ‚ñÑ‚ñà‚ñÜ‚ñÜ‚ñÇ‚ñá‚ñÑ‚ñÑ‚ñÜ‚ñÉ‚ñá‚ñÜ‚ñÖ‚ñÉ‚ñÉ‚ñÖ</td></tr><tr><td>validation score</td><td>‚ñÅ‚ñÅ‚ñÅ‚ñÅ</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>accuracy</td><td>0.85366</td></tr><tr><td>batch</td><td>940</td></tr><tr><td>f1</td><td>0.875</td></tr><tr><td>loss</td><td>0.04966</td></tr><tr><td>precision</td><td>0.89744</td></tr><tr><td>recall</td><td>0.85366</td></tr><tr><td>validation score</td><td>1.0</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">pious-aardvark-94</strong> at: <a href='https://wandb.ai/itu-nlp/NLP%20Project/runs/v8nrjjhe' target=\"_blank\">https://wandb.ai/itu-nlp/NLP%20Project/runs/v8nrjjhe</a><br/> View project at: <a href='https://wandb.ai/itu-nlp/NLP%20Project' target=\"_blank\">https://wandb.ai/itu-nlp/NLP%20Project</a><br/>Synced 5 W&B file(s), 4 media file(s), 4 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20240506_192946-v8nrjjhe\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "optimizer = optim.AdamW(model.parameters(), lr=LR)\n",
    "epoch = 1\n",
    "val_steps = 250\n",
    "for i in range(epoch):\n",
    "    for counter, batch in tqdm(enumerate(ds)):\n",
    "\n",
    "        for k, v in batch.items():\n",
    "            batch[k] = batch[k].to('cuda')\n",
    "\n",
    "        outputs = model(**batch)\n",
    "        logits = outputs.logits\n",
    "        \n",
    "        # Create attention mask\n",
    "        attention_mask = batch['attention_mask']\n",
    "        \n",
    "        # Compute categorical cross-entropy loss\n",
    "        active_loss = attention_mask.view(-1) == 1 # select only non-padded tokens (attended tokens)\n",
    "        active_logits = logits.view(-1, logits.shape[-1]) # get logits for all tokens\n",
    "        active_labels = torch.where(\n",
    "            active_loss, batch['labels'].view(-1), torch.tensor(model.config.pad_token_id).type_as(batch['labels'])\n",
    "        ) \n",
    "        loss = F.cross_entropy(active_logits, active_labels) # compute loss\n",
    "        \n",
    "        # Compute metrics\n",
    "        predictions = torch.argmax(logits, dim=2)\n",
    "        tp = ((batch['labels'] == predictions) & (batch['labels'] != model.config.pad_token_id)).sum()\n",
    "        tn = ((batch['labels'] != predictions) & (batch['labels'] != model.config.pad_token_id) & (predictions != model.config.pad_token_id)).sum()\n",
    "        total_labels = (batch['labels'] != model.config.pad_token_id).sum()\n",
    "        recall = tp / total_labels\n",
    "        acc = (tp + tn) / total_labels\n",
    "        precision = tp / (predictions != model.config.pad_token_id).sum()\n",
    "        f1 = (2 * recall * precision) / (recall + precision)\n",
    "\n",
    "        del batch #¬†free memory\n",
    "\n",
    "        # Step\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        # Log metrics\n",
    "        run.log({\"loss\": loss.item(), \"batch\": counter+1, 'recall': recall, 'accuracy': acc, 'precision': precision, 'f1': f1})\n",
    "        print(f'Epoch: [{i+1}/{epoch}], Batch [{counter+1}/{len(ds)}], Loss: {loss.item():.4f}, F1: {f1:.4f}, Recall: {recall:.4f}, Precision: {precision:.4f}, Accuracy: {acc:.4f}')\n",
    "\n",
    "        if not (counter % val_steps): #¬†validation\n",
    "            torch.save(model.state_dict(), f'base-{counter}.pt')\n",
    "            submission_df = submission(path='data/valid.jsonl', output_path='submission.csv')\n",
    "            answers = pd.read_csv('submission.csv')\n",
    "            val_score = acc_overall(submission_df.Sentence, answers.Sentence)\n",
    "            run.log({\"validation\": wandb.Table(dataframe=submission_df), 'validation score': val_score})\n",
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*save the model*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model, f'base_kelimeler-2.pt') # PEFT save function had a bug"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing the model on dev set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 100/100 [00:02<00:00, 47.75it/s]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>Sentence</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1001147</td>\n",
       "      <td>limeler ince √ºnl√º bulunduran bir fiille birlik...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1002343</td>\n",
       "      <td>bu yƒ±l t√ºrkiye olarak d√∂rd√ºnc√º kez katƒ±lacaƒüƒ±m...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1005460</td>\n",
       "      <td>bizim g√∂r√º≈ü√ºm√ºz ve uygulamalarƒ±mƒ±zda nusret f...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1002100</td>\n",
       "      <td>ankaralƒ±lar mƒ±sƒ±ra popcorn demez ve onu bir f...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1005445</td>\n",
       "      <td>daha sonralarƒ± i√ß i√ße bir ya≈üam s√ºrd√ºreceƒüim ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>1007513</td>\n",
       "      <td>sizce bir faydasƒ± olur mu ?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>1004524</td>\n",
       "      <td>yal√ßƒ±nkaya : bu yatƒ±rƒ±m sepeti kasƒ±m ayƒ±nda 6...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>1005141</td>\n",
       "      <td>√ßok y√ºksek : filmlerin romanlarƒ±n m√ºthi≈ü bilg...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>1003252</td>\n",
       "      <td>yenilenen stili ve kullanƒ±lan yeni teknolojile...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>1003791</td>\n",
       "      <td>antalya√ßok bulutlu22 ¬∞c / 15 ¬∞c</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100 rows √ó 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         ID                                           Sentence\n",
       "0   1001147  limeler ince √ºnl√º bulunduran bir fiille birlik...\n",
       "1   1002343  bu yƒ±l t√ºrkiye olarak d√∂rd√ºnc√º kez katƒ±lacaƒüƒ±m...\n",
       "2   1005460   bizim g√∂r√º≈ü√ºm√ºz ve uygulamalarƒ±mƒ±zda nusret f...\n",
       "3   1002100   ankaralƒ±lar mƒ±sƒ±ra popcorn demez ve onu bir f...\n",
       "4   1005445   daha sonralarƒ± i√ß i√ße bir ya≈üam s√ºrd√ºreceƒüim ...\n",
       "..      ...                                                ...\n",
       "95  1007513                       sizce bir faydasƒ± olur mu ? \n",
       "96  1004524   yal√ßƒ±nkaya : bu yatƒ±rƒ±m sepeti kasƒ±m ayƒ±nda 6...\n",
       "97  1005141   √ßok y√ºksek : filmlerin romanlarƒ±n m√ºthi≈ü bilg...\n",
       "98  1003252  yenilenen stili ve kullanƒ±lan yeni teknolojile...\n",
       "99  1003791                   antalya√ßok bulutlu22 ¬∞c / 15 ¬∞c \n",
       "\n",
       "[100 rows x 2 columns]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_df = submission(path='data/valid.jsonl', output_path='submission.csv')\n",
    "val_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'limeler ince √ºnl√º bulunduran bir fiille birlikte kullanƒ±lƒ±yorsa bir ulama sonucunda son sesteki bu kalƒ±n k sesi de ince okunabilmekte ve hatta gereƒüi yokken birle≈üik bile yazilabilmektedir . '"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_df.Sentence[0] #¬†check the first sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "yazilabilmektedir yazƒ±labilmektedir\n",
      "ayinda ayƒ±nda\n",
      "duzenlenecek d√ºzenlenecek\n",
      "turkiye t√ºrkiye\n",
      "odul √∂d√ºl\n",
      "cifte √ßifte\n",
      "hakkinda hakkƒ±nda\n",
      "yazin yazƒ±n\n",
      "beg'de beƒü'de\n",
      "in ƒ±n\n",
      "bilgisayar‚Äôdir bilgisayar‚Äôdƒ±r\n",
      "gecti ge√ßti\n",
      "dun d√ºn\n",
      "tum t√ºm\n",
      "uzmanlari uzmanlarƒ±\n",
      "yapildi yapƒ±ldƒ±\n",
      "nu n√º\n",
      "in ƒ±n\n",
      "olmasƒ±nki olmasƒ±nkƒ±\n",
      "d√∂n√ºkl√ºk donukluk\n",
      "kubali k√ºbalƒ±\n",
      "onlarin onlarƒ±n\n",
      "tercumanliƒüinƒ± terc√ºmanlƒ±ƒüƒ±nƒ±\n",
      "yapmis yapmƒ±≈ü\n",
      "ya≈üƒ±n yasin\n",
      "ge√ßmiste ge√ßmi≈üte\n",
      "bari≈ümakta barƒ±≈ümakta\n",
      "gormemistir g√∂rmemi≈ütir\n",
      "si√ßak sƒ±cak\n",
      "adƒ± adi\n",
      "dur. d√ºr.\n",
      "ornekti √∂rnekti\n",
      "aƒüasi aƒüasƒ±\n",
      "asƒ±retinin a≈üiretinin\n",
      "sƒ±mdiki ≈üimdiki\n",
      "adiyaman adƒ±yaman\n",
      "in ƒ±n\n",
      "adiyaman adƒ±yaman\n",
      "in ƒ±n\n",
      "yalin yalƒ±n\n",
      "bƒ±cimlerin bi√ßimlerin\n",
      "degil deƒüil\n",
      "i√¥ ƒ±√¥\n",
      "degil deƒüil\n",
      "taklƒ±t taklit\n",
      "yazilmasi yazƒ±lmasƒ±\n",
      "e≈üas esas\n",
      "alinmistir alƒ±nmƒ±≈ütƒ±r\n",
      "hayatin hayatƒ±n\n",
      "sacmaliklarla sa√ßmalƒ±klarla\n",
      "oldug√ºnu olduƒüunu\n",
      "duygularin duygularƒ±n\n",
      "unutuldug√ºnu unutulduƒüunu\n",
      "m3‚Äôun m3‚Äô√ºn\n",
      "uzerindeki √ºzerindeki\n",
      "icin i√ßin\n",
      "(hukumet (h√ºk√ºmet\n",
      "olmadi olmadƒ±\n",
      "dogulu doƒüulu\n",
      "cakkidƒ± √ßakkƒ±dƒ±\n",
      "ƒüibi gibi\n",
      "sarkiyla ≈üarkƒ±yla\n",
      "ortaligƒ± ortalƒ±ƒüƒ±\n",
      "yikar yƒ±kar\n",
      "gecer ge√ßer\n",
      "si≈üitemi sisitemi\n",
      "ask a≈ük\n",
      "yasamƒ± ya≈üamƒ±\n",
      "baktiƒüimƒ±zda baktƒ±ƒüƒ±mƒ±zda\n",
      "savas sava≈ü\n",
      "√∂n√ßesinde √∂ncesinde\n",
      "sunlar ≈üunlar\n",
      "boyle b√∂yle\n",
      "saskina ≈üa≈ükƒ±na\n",
      "donmu≈ütum d√∂nm√º≈üt√ºm\n",
      "balƒ±k√ßiliƒüƒ±na balƒ±k√ßƒ±lƒ±ƒüƒ±na\n",
      "sahipligƒ± sahipliƒüi\n",
      "yapiyor yapƒ±yor\n",
      "√∂n on\n",
      "eƒürƒ±kavuk eƒürikavuk\n",
      "genis geni≈ü\n",
      "rahatsiz rahatsƒ±z\n",
      "icin i√ßin\n",
      "elestƒ±ri ele≈ütiri\n",
      "ozellestƒ±rmeler √∂zelle≈ütirmeler\n",
      "taseronlasma ta≈üeronla≈üma\n",
      "basortul√ºler ba≈ü√∂rt√ºl√ºler\n",
      "unƒ±versitelerde √ºniversitelerde\n",
      "ozel √∂zel\n",
      "calisa√ßaklar √ßalƒ±≈üacaklar\n",
      "motorlarin motorlarƒ±n\n",
      "yanisƒ±ra yanƒ±sƒ±ra\n",
      "secenekleri se√ßenekleri\n",
      "kapatiyor kapatƒ±yor\n",
      "a√ßik a√ßƒ±k\n",
      "bugun bug√ºn\n",
      "uyeye √ºyeye\n",
      "kararina kararƒ±na\n",
      "degƒ±l deƒüil\n",
      "ayni aynƒ±\n",
      "hukumete h√ºk√ºmete\n",
      "buyuk b√ºy√ºk\n",
      "oldug√º olduƒüu\n",
      "goru≈üun√º g√∂r√º≈ü√ºn√º\n",
      "≈üavunurken savunurken\n",
      "muttefikleri m√ºttefikleri\n",
      "sur√ºklendiler s√ºr√ºklendiler\n",
      "seklinde ≈üeklinde\n",
      "ayirarak ayƒ±rarak\n",
      "giris giri≈ü\n",
      "yapinƒ±z yapƒ±nƒ±z\n",
      "cifte √ßifte\n",
      "yapalim yapalƒ±m\n",
      "turkiye t√ºrkiye\n",
      "hukumet h√ºk√ºmet\n",
      "yapsin yapsƒ±n\n",
      "u √º\n",
      "art√ºkl√º artuklu\n",
      "kalƒ±ntƒ±lari kalƒ±ntƒ±larƒ±\n",
      "oƒü√ºnden og√ºnden\n",
      "c√∂k √ßok\n",
      "guzel g√ºzel\n",
      "beraberligƒ±miz beraberliƒüimiz\n",
      "evliligƒ± evliliƒüi\n",
      "hic hi√ß\n",
      "du≈ü√ºnmuyorduk d√º≈ü√ºnm√ºyorduk\n",
      "teror ter√∂r\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.9202762084118016"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "answers = pd.read_csv('data/valid_answers.csv')\n",
    "val = acc_overall(val_df.Sentence, answers.Sentence)\n",
    "val"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Submission on test set - Real World"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1157/1157 [00:21<00:00, 52.92it/s]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>Sentence</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>tr ekonomi ve politika haberleri t√ºrkiye nin ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>√ºye giri≈üi</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>son g√ºncelleme 12:12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>ƒ∞mralƒ± Mit g√∂r√º≈ümesi ihtiya√ß duyulduk√ßa oluyor</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>Suriye deki silahlƒ± selefi muhalifler yeni ku...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1152</th>\n",
       "      <td>1152</td>\n",
       "      <td>Y√ºreƒüir Adana ilimize ait ≈üirin bir il√ßedir</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1153</th>\n",
       "      <td>1153</td>\n",
       "      <td>y√ºze g√ºl√ºc√ºl√ºƒü√ºn at oynattƒ±ƒüƒ± bir aydƒ±nlar ort...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1154</th>\n",
       "      <td>1154</td>\n",
       "      <td>zavallƒ± adamƒ± oracƒ±kta astƒ±lar ve hi√ß kimse se...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1155</th>\n",
       "      <td>1155</td>\n",
       "      <td>zengin √ßocuklarƒ±na arƒ±z m√ºnasebetsizlikler fak...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1156</th>\n",
       "      <td>1156</td>\n",
       "      <td>senin a√ßƒ±n hepimizin acƒ±sƒ±dƒ±r</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1157 rows √ó 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        ID                                           Sentence\n",
       "0        0   tr ekonomi ve politika haberleri t√ºrkiye nin ...\n",
       "1        1                                        √ºye giri≈üi \n",
       "2        2                              son g√ºncelleme 12:12 \n",
       "3        3    ƒ∞mralƒ± Mit g√∂r√º≈ümesi ihtiya√ß duyulduk√ßa oluyor \n",
       "4        4   Suriye deki silahlƒ± selefi muhalifler yeni ku...\n",
       "...    ...                                                ...\n",
       "1152  1152       Y√ºreƒüir Adana ilimize ait ≈üirin bir il√ßedir \n",
       "1153  1153  y√ºze g√ºl√ºc√ºl√ºƒü√ºn at oynattƒ±ƒüƒ± bir aydƒ±nlar ort...\n",
       "1154  1154  zavallƒ± adamƒ± oracƒ±kta astƒ±lar ve hi√ß kimse se...\n",
       "1155  1155  zengin √ßocuklarƒ±na arƒ±z m√ºnasebetsizlikler fak...\n",
       "1156  1156                     senin a√ßƒ±n hepimizin acƒ±sƒ±dƒ±r \n",
       "\n",
       "[1157 rows x 2 columns]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "submission_df = submission(path='data/test.jsonl', output_path='submission.csv')\n",
    "submission_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def try_it(text, model=model):\n",
    "    sample = test_mask([text])\n",
    "    return rewrite(model, sample[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'nasƒ±lsƒ±n bu ak≈üam, bir ≈üeyler i√ßmek ister misin'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "try_it('nasilsin bu aksam, bir seyler icmek ister misin')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
